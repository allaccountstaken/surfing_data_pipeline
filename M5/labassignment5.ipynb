{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 5: Web Scraping\n",
    "## DS 6001: Practice and Application of Data Science\n",
    "\n",
    "### Instructions\n",
    "Please answer the following questions as completely as possible using text, code, and the results of code as needed. Format your answers in a Jupyter notebook. To receive full credit, make sure you address every part of the problem, and make sure your document is formatted in a clean and professional way.\n",
    "\n",
    "For the following problems, you will be scraping http://books.toscrape.com/. This website is a fake book retailer, designed to mimic the design of many retail websites. It exists solely to help students practice web-scraping, so there aren’t going to be any ethical concerns with this particular exercise, and there shouldn’t be any issues with rate limits or other gates that could prevent web-scraping. Take a moment and look at this website, so that you know what you will be working with.\n",
    "\n",
    "Your goal is to generate a dataframe with four columns: one for the title, one for the price, one for the star-rating, and one or the book cover JPEG’s URL. The dataframe will also 1000 rows, one for each of the 1000 books listed on the 50 pages of this website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0\n",
    "Import the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "sys.tracebacklimit = 0 # turn off the error tracebacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "Pull the HTML code from http://books.toscrape.com/. Make sure you provide a user agent string. Then parse this HTML code and save the parsed code as a separate Python variable. [3 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target URL of the book store\n",
    "url = \"http://books.toscrape.com/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get response from the server\n",
    "response = requests.get(url, headers={'User-agent':'Dima Mikhalov'})\n",
    "\n",
    "# Check status, 200 expected\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<!--[if lt IE 7]>      <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]-->\\n<!--[if IE 7]>         <html lang=\"en-us\" class=\"no-js lt-ie9 lt-ie8\"> <![endif]-->\\n<!--[if IE 8]>         <html lang=\"en-us\" class=\"no-js lt-ie9\"> <![endif]-->\\n<!--[if gt IE 8]><!--> <html lang=\"en-us\" class=\"no-js\"> <!--<![endif]-->\\n    <head>\\n        <title>\\n    All products | Books to Scrape - Sandbox\\n</title>\\n\\n        <meta http-equiv=\"content-type\" content=\"text/html; charset=UTF-8\" />\\n        <meta name=\"created\" content=\"24th Jun 2016 09:29\" />\\n        <meta name=\"description\" content=\"\" />\\n        <meta name=\"viewport\" content=\"width=device-width\" />\\n        <meta name=\"robots\" content=\"NOARCHIVE,NOCACHE\" />\\n\\n        <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->\\n        <!--[if lt IE 9]>\\n        <script src=\"//html5shim.googlecode.com/svn/trunk/html5.js\"></script>\\n        <![endif]-->\\n\\n        \\n            <link rel=\"shortcut icon\" href=\"static/oscar/favicon.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View HTML as text string \n",
    "response.text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse HTML tags\n",
    "books = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2\n",
    "Extract all 20 of the book titles and save them in a list. [2 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Light in the ...',\n",
       " 'Tipping the Velvet',\n",
       " 'Soumission',\n",
       " 'Sharp Objects',\n",
       " 'Sapiens: A Brief History ...',\n",
       " 'The Requiem Red',\n",
       " 'The Dirty Little Secrets ...',\n",
       " 'The Coming Woman: A ...',\n",
       " 'The Boys in the ...',\n",
       " 'The Black Maria',\n",
       " 'Starving Hearts (Triangular Trade ...',\n",
       " \"Shakespeare's Sonnets\",\n",
       " 'Set Me Free',\n",
       " \"Scott Pilgrim's Precious Little ...\",\n",
       " 'Rip it Up and ...',\n",
       " 'Our Band Could Be ...',\n",
       " 'Olio',\n",
       " 'Mesaerion: The Best Science ...',\n",
       " 'Libertarianism for Beginners',\n",
       " \"It's Only the Himalayas\"]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In a for loop check all `h3` tags and store .string\n",
    "titles = [i.string for i in books.find_all('h3')]\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many elements were added to the lits\n",
    "len(titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "Extract the price of each of the 20 books and save these prices in a list. (The prices are listed in British pounds, and include the £ symbol. Remove the £ symbols: if you’ve saved the prices in a list named `prices`, then the following code should work: `prices = [s.replace('Â£', '') for s in prices]`.) [2 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['51.77',\n",
       " '53.74',\n",
       " '50.10',\n",
       " '47.82',\n",
       " '54.23',\n",
       " '22.65',\n",
       " '33.34',\n",
       " '17.93',\n",
       " '22.60',\n",
       " '52.15',\n",
       " '13.99',\n",
       " '20.66',\n",
       " '17.46',\n",
       " '52.29',\n",
       " '35.02',\n",
       " '57.25',\n",
       " '23.88',\n",
       " '37.59',\n",
       " '51.33',\n",
       " '45.17']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a list of corrected prices\n",
    "prices = [i.string.replace('Â£', '') for i in books.find_all('p')[1::3]] # [start:stop:step]\n",
    "prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many elements were added to the lits\n",
    "len(prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "Extract the star level ratings for the 20 books. [Hint: for tags such as `<p class=\"star-rating One\">` in which the class has a space, the class is actually a list in which the first item in the list is `\"star-rating\"` and the second item in the list is `\"One\"`. It's possible to search on either item in this list.] [3 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Three',\n",
       " 'One',\n",
       " 'One',\n",
       " 'Four',\n",
       " 'Five',\n",
       " 'One',\n",
       " 'Four',\n",
       " 'Three',\n",
       " 'Four',\n",
       " 'One',\n",
       " 'Two',\n",
       " 'Four',\n",
       " 'Five',\n",
       " 'Five',\n",
       " 'Five',\n",
       " 'Three',\n",
       " 'One',\n",
       " 'One',\n",
       " 'Two',\n",
       " 'Two']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build a list of # stars:\n",
    "ratings = [i.attrs.get('class')[1] for i in books.find_all('p', 'star-rating')]\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many elements were added to the lits\n",
    "len(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "Extract the URLs for the JPEG thumbnail images that show the covers of the 20 books. (Maybe we want to mine the images to build models that predict the star level, literally judging books by their covers.) [2 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg',\n",
       " 'media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f4a1c.jpg',\n",
       " 'media/cache/3e/ef/3eef99c9d9adef34639f510662022830.jpg',\n",
       " 'media/cache/32/51/3251cf3a3412f53f339e42cac2134093.jpg',\n",
       " 'media/cache/be/a5/bea5697f2534a2f86a3ef27b5a8c12a6.jpg',\n",
       " 'media/cache/68/33/68339b4c9bc034267e1da611ab3b34f8.jpg',\n",
       " 'media/cache/92/27/92274a95b7c251fea59a2b8a78275ab4.jpg',\n",
       " 'media/cache/3d/54/3d54940e57e662c4dd1f3ff00c78cc64.jpg',\n",
       " 'media/cache/66/88/66883b91f6804b2323c8369331cb7dd1.jpg',\n",
       " 'media/cache/58/46/5846057e28022268153beff6d352b06c.jpg',\n",
       " 'media/cache/be/f4/bef44da28c98f905a3ebec0b87be8530.jpg',\n",
       " 'media/cache/10/48/1048f63d3b5061cd2f424d20b3f9b666.jpg',\n",
       " 'media/cache/5b/88/5b88c52633f53cacf162c15f4f823153.jpg',\n",
       " 'media/cache/94/b1/94b1b8b244bce9677c2f29ccc890d4d2.jpg',\n",
       " 'media/cache/81/c4/81c4a973364e17d01f217e1188253d5e.jpg',\n",
       " 'media/cache/54/60/54607fe8945897cdcced0044103b10b6.jpg',\n",
       " 'media/cache/55/33/553310a7162dfbc2c6d19a84da0df9e1.jpg',\n",
       " 'media/cache/09/a3/09a3aef48557576e1a85ba7efea8ecb7.jpg',\n",
       " 'media/cache/0b/bc/0bbcd0a6f4bcd81ccb1049a52736406e.jpg',\n",
       " 'media/cache/27/a5/27a53d0bb95bdd88288eaf66c9230d7e.jpg']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build and check the list of URLS to thumbnail images:\n",
    "images = [i.attrs.get('src') for i in books.find_all(\"img\" )]\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many elements were added to the lits\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6\n",
    "Create a dataframe with one row for each of the 20 books, and the book titles, prices, star ratings, and cover JPEG URLs as the four columns. [2 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Covers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Light in the ...</td>\n",
       "      <td>51.77</td>\n",
       "      <td>Three</td>\n",
       "      <td>media/cache/2c/da/2cdad67c44b002e7ead0cc35693c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tipping the Velvet</td>\n",
       "      <td>53.74</td>\n",
       "      <td>One</td>\n",
       "      <td>media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Soumission</td>\n",
       "      <td>50.10</td>\n",
       "      <td>One</td>\n",
       "      <td>media/cache/3e/ef/3eef99c9d9adef34639f51066202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sharp Objects</td>\n",
       "      <td>47.82</td>\n",
       "      <td>Four</td>\n",
       "      <td>media/cache/32/51/3251cf3a3412f53f339e42cac213...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sapiens: A Brief History ...</td>\n",
       "      <td>54.23</td>\n",
       "      <td>Five</td>\n",
       "      <td>media/cache/be/a5/bea5697f2534a2f86a3ef27b5a8c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Requiem Red</td>\n",
       "      <td>22.65</td>\n",
       "      <td>One</td>\n",
       "      <td>media/cache/68/33/68339b4c9bc034267e1da611ab3b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Dirty Little Secrets ...</td>\n",
       "      <td>33.34</td>\n",
       "      <td>Four</td>\n",
       "      <td>media/cache/92/27/92274a95b7c251fea59a2b8a7827...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Coming Woman: A ...</td>\n",
       "      <td>17.93</td>\n",
       "      <td>Three</td>\n",
       "      <td>media/cache/3d/54/3d54940e57e662c4dd1f3ff00c78...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Boys in the ...</td>\n",
       "      <td>22.60</td>\n",
       "      <td>Four</td>\n",
       "      <td>media/cache/66/88/66883b91f6804b2323c8369331cb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Black Maria</td>\n",
       "      <td>52.15</td>\n",
       "      <td>One</td>\n",
       "      <td>media/cache/58/46/5846057e28022268153beff6d352...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Starving Hearts (Triangular Trade ...</td>\n",
       "      <td>13.99</td>\n",
       "      <td>Two</td>\n",
       "      <td>media/cache/be/f4/bef44da28c98f905a3ebec0b87be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Shakespeare's Sonnets</td>\n",
       "      <td>20.66</td>\n",
       "      <td>Four</td>\n",
       "      <td>media/cache/10/48/1048f63d3b5061cd2f424d20b3f9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Set Me Free</td>\n",
       "      <td>17.46</td>\n",
       "      <td>Five</td>\n",
       "      <td>media/cache/5b/88/5b88c52633f53cacf162c15f4f82...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Scott Pilgrim's Precious Little ...</td>\n",
       "      <td>52.29</td>\n",
       "      <td>Five</td>\n",
       "      <td>media/cache/94/b1/94b1b8b244bce9677c2f29ccc890...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Rip it Up and ...</td>\n",
       "      <td>35.02</td>\n",
       "      <td>Five</td>\n",
       "      <td>media/cache/81/c4/81c4a973364e17d01f217e118825...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Our Band Could Be ...</td>\n",
       "      <td>57.25</td>\n",
       "      <td>Three</td>\n",
       "      <td>media/cache/54/60/54607fe8945897cdcced0044103b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Olio</td>\n",
       "      <td>23.88</td>\n",
       "      <td>One</td>\n",
       "      <td>media/cache/55/33/553310a7162dfbc2c6d19a84da0d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Mesaerion: The Best Science ...</td>\n",
       "      <td>37.59</td>\n",
       "      <td>One</td>\n",
       "      <td>media/cache/09/a3/09a3aef48557576e1a85ba7efea8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Libertarianism for Beginners</td>\n",
       "      <td>51.33</td>\n",
       "      <td>Two</td>\n",
       "      <td>media/cache/0b/bc/0bbcd0a6f4bcd81ccb1049a52736...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>It's Only the Himalayas</td>\n",
       "      <td>45.17</td>\n",
       "      <td>Two</td>\n",
       "      <td>media/cache/27/a5/27a53d0bb95bdd88288eaf66c923...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Title  Price Rating  \\\n",
       "0                      A Light in the ...  51.77  Three   \n",
       "1                      Tipping the Velvet  53.74    One   \n",
       "2                              Soumission  50.10    One   \n",
       "3                           Sharp Objects  47.82   Four   \n",
       "4            Sapiens: A Brief History ...  54.23   Five   \n",
       "5                         The Requiem Red  22.65    One   \n",
       "6            The Dirty Little Secrets ...  33.34   Four   \n",
       "7                 The Coming Woman: A ...  17.93  Three   \n",
       "8                     The Boys in the ...  22.60   Four   \n",
       "9                         The Black Maria  52.15    One   \n",
       "10  Starving Hearts (Triangular Trade ...  13.99    Two   \n",
       "11                  Shakespeare's Sonnets  20.66   Four   \n",
       "12                            Set Me Free  17.46   Five   \n",
       "13    Scott Pilgrim's Precious Little ...  52.29   Five   \n",
       "14                      Rip it Up and ...  35.02   Five   \n",
       "15                  Our Band Could Be ...  57.25  Three   \n",
       "16                                   Olio  23.88    One   \n",
       "17        Mesaerion: The Best Science ...  37.59    One   \n",
       "18           Libertarianism for Beginners  51.33    Two   \n",
       "19                It's Only the Himalayas  45.17    Two   \n",
       "\n",
       "                                               Covers  \n",
       "0   media/cache/2c/da/2cdad67c44b002e7ead0cc35693c...  \n",
       "1   media/cache/26/0c/260c6ae16bce31c8f8c95daddd9f...  \n",
       "2   media/cache/3e/ef/3eef99c9d9adef34639f51066202...  \n",
       "3   media/cache/32/51/3251cf3a3412f53f339e42cac213...  \n",
       "4   media/cache/be/a5/bea5697f2534a2f86a3ef27b5a8c...  \n",
       "5   media/cache/68/33/68339b4c9bc034267e1da611ab3b...  \n",
       "6   media/cache/92/27/92274a95b7c251fea59a2b8a7827...  \n",
       "7   media/cache/3d/54/3d54940e57e662c4dd1f3ff00c78...  \n",
       "8   media/cache/66/88/66883b91f6804b2323c8369331cb...  \n",
       "9   media/cache/58/46/5846057e28022268153beff6d352...  \n",
       "10  media/cache/be/f4/bef44da28c98f905a3ebec0b87be...  \n",
       "11  media/cache/10/48/1048f63d3b5061cd2f424d20b3f9...  \n",
       "12  media/cache/5b/88/5b88c52633f53cacf162c15f4f82...  \n",
       "13  media/cache/94/b1/94b1b8b244bce9677c2f29ccc890...  \n",
       "14  media/cache/81/c4/81c4a973364e17d01f217e118825...  \n",
       "15  media/cache/54/60/54607fe8945897cdcced0044103b...  \n",
       "16  media/cache/55/33/553310a7162dfbc2c6d19a84da0d...  \n",
       "17  media/cache/09/a3/09a3aef48557576e1a85ba7efea8...  \n",
       "18  media/cache/0b/bc/0bbcd0a6f4bcd81ccb1049a52736...  \n",
       "19  media/cache/27/a5/27a53d0bb95bdd88288eaf66c923...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First create a dictionary to define the structure:\n",
    "my_dict = {'Title': titles,\n",
    "           'Price': prices,\n",
    "           'Rating':ratings,\n",
    "           'Covers':images,\n",
    "          }\n",
    "\n",
    "# Convert dictionary to a DataFrame for presentation and analysis\n",
    "pd.DataFrame(my_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7\n",
    "Create a function that takes the URL of the webpage to scrape as an input, applies the code you wrote for questions 1 through 6, and generates the dataframe from question 6 as the output. [3 points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the function, note - user-name is also required as per #1 requirement\n",
    "def books_scraper(tagret_url, user_agent):\n",
    "    response = requests.get(tagret_url, headers={'User-agent': user_agent})\n",
    "    books = BeautifulSoup(response.text)\n",
    "    titles = [i.string for i in books.find_all('h3')]\n",
    "    prices = [i.string.replace('Â£', '') for i in books.find_all('p')[1::3]]\n",
    "    ratings = [i.attrs.get('class')[1] for i in books.find_all('p', 'star-rating')]\n",
    "    images = [i.attrs.get('src') for i in books.find_all(\"img\" )]\n",
    "    my_dict = {'Title': titles, 'Price': prices, 'Rating':ratings, 'Covers':images,}\n",
    "    results = pd.DataFrame(my_dict)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8\n",
    "Notice that there are many pages to http://books.toscrape.com/. When you click on “Next” in the bottom-right corner of the screen, it takes you to http://books.toscrape.com/catalogue/page-2.html. The front page is the same as http://books.toscrape.com/catalogue/page-1.html, and there are 50 total pages.\n",
    "\n",
    "Write a loop that uses the function you wrote in question 7 to scrape each of the 50 pages, and append each of these data frames together. If you write this loop correctly, your dataframe will have 1000 rows (20 books on each of the 50 pages). \n",
    "\n",
    "Some hints:\n",
    "\n",
    "* Typing `new_df = pd.DataFrame()` with nothing in the parentheses will create an empty data frame on which new data can be appended.\n",
    "\n",
    "* There are many loops you can use, but the most straightforward one is a for-values loop that counts from 1 to 50. In Python, you can initialize such a loop with for i in range(1, 51):, and indenting every line below it that belongs inside the loop. Inside the loop, the letter i is now a stand-in for the number currently being considered.\n",
    "\n",
    "* You will need to figure out how to replace the number in URLs like http://books.toscrape.com/catalogue/page-2.html with the number currently under consideration in the loop. You might need the `str()` function, which turns numeric values into strings.\n",
    "\n",
    "[3 points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 --checking next url: http://books.toscrape.com/catalogue/page-1.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-125-fdc3dc565707>:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  new_df = new_df.append(books_scraper(next_url, \"Dima\"), ignore_index = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2 --checking next url: http://books.toscrape.com/catalogue/page-2.html\n",
      "Step 3 --checking next url: http://books.toscrape.com/catalogue/page-3.html\n",
      "Step 4 --checking next url: http://books.toscrape.com/catalogue/page-4.html\n",
      "Step 5 --checking next url: http://books.toscrape.com/catalogue/page-5.html\n",
      "Step 6 --checking next url: http://books.toscrape.com/catalogue/page-6.html\n",
      "Step 7 --checking next url: http://books.toscrape.com/catalogue/page-7.html\n",
      "Step 8 --checking next url: http://books.toscrape.com/catalogue/page-8.html\n",
      "Step 9 --checking next url: http://books.toscrape.com/catalogue/page-9.html\n",
      "Step 10 --checking next url: http://books.toscrape.com/catalogue/page-10.html\n",
      "Step 11 --checking next url: http://books.toscrape.com/catalogue/page-11.html\n",
      "Step 12 --checking next url: http://books.toscrape.com/catalogue/page-12.html\n",
      "Step 13 --checking next url: http://books.toscrape.com/catalogue/page-13.html\n",
      "Step 14 --checking next url: http://books.toscrape.com/catalogue/page-14.html\n",
      "Step 15 --checking next url: http://books.toscrape.com/catalogue/page-15.html\n",
      "Step 16 --checking next url: http://books.toscrape.com/catalogue/page-16.html\n",
      "Step 17 --checking next url: http://books.toscrape.com/catalogue/page-17.html\n",
      "Step 18 --checking next url: http://books.toscrape.com/catalogue/page-18.html\n",
      "Step 19 --checking next url: http://books.toscrape.com/catalogue/page-19.html\n",
      "Step 20 --checking next url: http://books.toscrape.com/catalogue/page-20.html\n",
      "Step 21 --checking next url: http://books.toscrape.com/catalogue/page-21.html\n",
      "Step 22 --checking next url: http://books.toscrape.com/catalogue/page-22.html\n",
      "Step 23 --checking next url: http://books.toscrape.com/catalogue/page-23.html\n",
      "Step 24 --checking next url: http://books.toscrape.com/catalogue/page-24.html\n",
      "Step 25 --checking next url: http://books.toscrape.com/catalogue/page-25.html\n",
      "Step 26 --checking next url: http://books.toscrape.com/catalogue/page-26.html\n",
      "Step 27 --checking next url: http://books.toscrape.com/catalogue/page-27.html\n",
      "Step 28 --checking next url: http://books.toscrape.com/catalogue/page-28.html\n",
      "Step 29 --checking next url: http://books.toscrape.com/catalogue/page-29.html\n",
      "Step 30 --checking next url: http://books.toscrape.com/catalogue/page-30.html\n",
      "Step 31 --checking next url: http://books.toscrape.com/catalogue/page-31.html\n",
      "Step 32 --checking next url: http://books.toscrape.com/catalogue/page-32.html\n",
      "Step 33 --checking next url: http://books.toscrape.com/catalogue/page-33.html\n",
      "Step 34 --checking next url: http://books.toscrape.com/catalogue/page-34.html\n",
      "Step 35 --checking next url: http://books.toscrape.com/catalogue/page-35.html\n",
      "Step 36 --checking next url: http://books.toscrape.com/catalogue/page-36.html\n",
      "Step 37 --checking next url: http://books.toscrape.com/catalogue/page-37.html\n",
      "Step 38 --checking next url: http://books.toscrape.com/catalogue/page-38.html\n",
      "Step 39 --checking next url: http://books.toscrape.com/catalogue/page-39.html\n",
      "Step 40 --checking next url: http://books.toscrape.com/catalogue/page-40.html\n",
      "Step 41 --checking next url: http://books.toscrape.com/catalogue/page-41.html\n",
      "Step 42 --checking next url: http://books.toscrape.com/catalogue/page-42.html\n",
      "Step 43 --checking next url: http://books.toscrape.com/catalogue/page-43.html\n",
      "Step 44 --checking next url: http://books.toscrape.com/catalogue/page-44.html\n",
      "Step 45 --checking next url: http://books.toscrape.com/catalogue/page-45.html\n",
      "Step 46 --checking next url: http://books.toscrape.com/catalogue/page-46.html\n",
      "Step 47 --checking next url: http://books.toscrape.com/catalogue/page-47.html\n",
      "Step 48 --checking next url: http://books.toscrape.com/catalogue/page-48.html\n",
      "Step 49 --checking next url: http://books.toscrape.com/catalogue/page-49.html\n",
      "Step 50 --checking next url: http://books.toscrape.com/catalogue/page-50.html\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Title   1000 non-null   object\n",
      " 1   Price   1000 non-null   object\n",
      " 2   Rating  1000 non-null   object\n",
      " 3   Covers  1000 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 31.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Placeholder for the results and root URL\n",
    "new_df = pd.DataFrame()\n",
    "\n",
    "# For loop to scrape multile pages:\n",
    "for i in range(1, 51):\n",
    "    next_url = 'http://books.toscrape.com/catalogue/page-' + str(i) + '.html'\n",
    "    print(\"Step\", i, \"--checking next url:\", next_url)\n",
    "    r = requests.get(next_url)\n",
    "    \n",
    "    # Check if response is okay, if not then break\n",
    "    if r.status_code != 200:\n",
    "        break\n",
    "    \n",
    "    # If is okay, use books_scraper function and append the results\n",
    "    else: \n",
    "        new_df = new_df.append(books_scraper(next_url, \"Dima\"), ignore_index = True)\n",
    "        # Check next page\n",
    "        i += 1\n",
    "\n",
    "# Report meta data for the resulst\n",
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
