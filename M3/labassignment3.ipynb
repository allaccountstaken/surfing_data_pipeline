{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 3: How to Load, Convert, and Write JSON Files in Python\n",
    "## DS 6001: Practice and Application of Data Science\n",
    "\n",
    "### Instructions\n",
    "Please answer the following questions as completely as possible using text, code, and the results of code as needed. Format your answers in a Jupyter notebook. To receive full credit, make sure you address every part of the problem, and make sure your document is formatted in a clean and professional way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0\n",
    "Import the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "sys.tracebacklimit = 0 # turn off the error tracebacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 \n",
    "JSON and CSV are both text-based formats for the storage of data. It's possible to open either one in a plain text editor. Given this similarity, why does a CSV file usually take less memory than a JSON formatted file for the same data? Under what conditions could a JSON file be smaller in memory than a CSV file for the same data? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1: \n",
    "Generally speaking, JSON tends to be bigger because all the keys are repeated in a nested structure. One likely condition when JSON may be smaller than CSV is when there are lot of missing values in the data. JSON will simply omit keys when corresponding values are missing, but CSV will still store NA or an extra comma to indicate that the value is missing. When the size of the data is large, this will lead to a noticeably larger CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "NASA has a dataset of all meteorites that have fallen to Earth between the years A.D. 860 and 2013. The data contain the name of each meteorite, along with the coordinates of the place where the meteorite hit, the mass of the meteorite, and the date of the collison. The data is stored as a JSON here: https://data.nasa.gov/resource/y77d-th95.json\n",
    "\n",
    "Look at the data in your web-browser and explain which strategy for loading the JSON into Python makes the most sense and why. \n",
    "\n",
    "Then write and run the code that will work for loading the data into Python. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2:\n",
    "Strategy:\n",
    "JSON has some nesting when it comes to \"geolocation\", so use normalize with `max_level=1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First create `req` object and check its status:\n",
    "url = \"https://data.nasa.gov/resource/y77d-th95.json\"\n",
    "req = requests.get(url, headers = {'User-agent':'Dima'})\n",
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Aachen',\n",
       "  'id': '1',\n",
       "  'nametype': 'Valid',\n",
       "  'recclass': 'L5',\n",
       "  'mass': '21',\n",
       "  'fall': 'Fell',\n",
       "  'year': '1880-01-01T00:00:00.000',\n",
       "  'reclat': '50.775000',\n",
       "  'reclong': '6.083330',\n",
       "  'geolocation': {'type': 'Point', 'coordinates': [6.08333, 50.775]}},\n",
       " {'name': 'Aarhus',\n",
       "  'id': '2',\n",
       "  'nametype': 'Valid',\n",
       "  'recclass': 'H6',\n",
       "  'mass': '720',\n",
       "  'fall': 'Fell',\n",
       "  'year': '1951-01-01T00:00:00.000',\n",
       "  'reclat': '56.183330',\n",
       "  'reclong': '10.233330',\n",
       "  'geolocation': {'type': 'Point', 'coordinates': [10.23333, 56.18333]}}]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second, load text into the memory and check the nested structure:\n",
    "nasa_json = json.loads(req.text)\n",
    "nasa_json[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>nametype</th>\n",
       "      <th>recclass</th>\n",
       "      <th>mass</th>\n",
       "      <th>fall</th>\n",
       "      <th>year</th>\n",
       "      <th>reclat</th>\n",
       "      <th>reclong</th>\n",
       "      <th>geolocation.type</th>\n",
       "      <th>geolocation.coordinates</th>\n",
       "      <th>:@computed_region_cbhk_fwbd</th>\n",
       "      <th>:@computed_region_nnqa_25f4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aachen</td>\n",
       "      <td>1</td>\n",
       "      <td>Valid</td>\n",
       "      <td>L5</td>\n",
       "      <td>21</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1880-01-01T00:00:00.000</td>\n",
       "      <td>50.775000</td>\n",
       "      <td>6.083330</td>\n",
       "      <td>Point</td>\n",
       "      <td>[6.08333, 50.775]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aarhus</td>\n",
       "      <td>2</td>\n",
       "      <td>Valid</td>\n",
       "      <td>H6</td>\n",
       "      <td>720</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1951-01-01T00:00:00.000</td>\n",
       "      <td>56.183330</td>\n",
       "      <td>10.233330</td>\n",
       "      <td>Point</td>\n",
       "      <td>[10.23333, 56.18333]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abee</td>\n",
       "      <td>6</td>\n",
       "      <td>Valid</td>\n",
       "      <td>EH4</td>\n",
       "      <td>107000</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1952-01-01T00:00:00.000</td>\n",
       "      <td>54.216670</td>\n",
       "      <td>-113.000000</td>\n",
       "      <td>Point</td>\n",
       "      <td>[-113, 54.21667]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acapulco</td>\n",
       "      <td>10</td>\n",
       "      <td>Valid</td>\n",
       "      <td>Acapulcoite</td>\n",
       "      <td>1914</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1976-01-01T00:00:00.000</td>\n",
       "      <td>16.883330</td>\n",
       "      <td>-99.900000</td>\n",
       "      <td>Point</td>\n",
       "      <td>[-99.9, 16.88333]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Achiras</td>\n",
       "      <td>370</td>\n",
       "      <td>Valid</td>\n",
       "      <td>L6</td>\n",
       "      <td>780</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1902-01-01T00:00:00.000</td>\n",
       "      <td>-33.166670</td>\n",
       "      <td>-64.950000</td>\n",
       "      <td>Point</td>\n",
       "      <td>[-64.95, -33.16667]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name   id nametype     recclass    mass  fall                     year  \\\n",
       "0    Aachen    1    Valid           L5      21  Fell  1880-01-01T00:00:00.000   \n",
       "1    Aarhus    2    Valid           H6     720  Fell  1951-01-01T00:00:00.000   \n",
       "2      Abee    6    Valid          EH4  107000  Fell  1952-01-01T00:00:00.000   \n",
       "3  Acapulco   10    Valid  Acapulcoite    1914  Fell  1976-01-01T00:00:00.000   \n",
       "4   Achiras  370    Valid           L6     780  Fell  1902-01-01T00:00:00.000   \n",
       "\n",
       "       reclat      reclong geolocation.type geolocation.coordinates  \\\n",
       "0   50.775000     6.083330            Point       [6.08333, 50.775]   \n",
       "1   56.183330    10.233330            Point    [10.23333, 56.18333]   \n",
       "2   54.216670  -113.000000            Point        [-113, 54.21667]   \n",
       "3   16.883330   -99.900000            Point       [-99.9, 16.88333]   \n",
       "4  -33.166670   -64.950000            Point     [-64.95, -33.16667]   \n",
       "\n",
       "  :@computed_region_cbhk_fwbd :@computed_region_nnqa_25f4  \n",
       "0                         NaN                         NaN  \n",
       "1                         NaN                         NaN  \n",
       "2                         NaN                         NaN  \n",
       "3                         NaN                         NaN  \n",
       "4                         NaN                         NaN  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use pandas normalize with 1 additional level of nesting:\n",
    "pd.json_normalize(nasa_json, max_level=1)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "The textbook chapter for this module shows, as an example, how to pull data in JSON format from Reddit's top 25 posts on [/r/popular](https://www.reddit.com/r/popular/top/). The steps outlined there pull all of the features in the data into the dataframe, resulting in a dataframe with 172 columns. \n",
    "\n",
    "If we only wanted a few features, then looping across elements of the JSON list itself and extracting only the data we want may be a more efficient approach.\n",
    "\n",
    "Use looping - and not `pd.read_json()` or `pd.json_normalize()` - to create a dataframe with 25 rows (one for each of the top 25 posts), and only columns for `subreddit`, `title`, `ups`, and `created_utc`. The JSON file exists at http://www.reddit.com/r/popular/top.json, and don't forget to specify `headers = {'User-agent': 'DS6001'}` within `requests.get()`. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check status of the request\n",
    "url2 = 'http://www.reddit.com/r/popular/top.json'\n",
    "req2 = requests.get(url2, headers = {'User-agent': 'DS6001'})\n",
    "req2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text into the memory\n",
    "reddit_json = json.loads(req2.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MadeMeSmile'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For the first record, check 'subreddit'\n",
    "reddit_json['data']['children'][0]['data']['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Make sure the human is following our group, we must keep it alive and safe'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same, check 'title':\n",
    "reddit_json['data']['children'][0]['data']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118314"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check 'ups'\n",
    "reddit_json['data']['children'][0]['data']['ups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1644378607.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check 'created_utc' fieled:\n",
    "reddit_json['data']['children'][0]['data']['created_utc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, repeat the above steps in loops for 25 records, add results to a dataframe\n",
    "reddit_df = pd.DataFrame(columns=['subreddit', 'title', 'ups', 'created_utc'])\n",
    "reddit_df.subreddit = [j['data']['subreddit'] for j in reddit_json['data']['children']][:25]\n",
    "reddit_df.title = [j['data']['title'] for j in reddit_json['data']['children']][:25]\n",
    "reddit_df.ups = [j['data']['ups'] for j in reddit_json['data']['children']][:25]\n",
    "reddit_df.created_utc = [j['data']['created_utc'] for j in reddit_json['data']['children']][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>ups</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MadeMeSmile</td>\n",
       "      <td>Make sure the human is following our group, we...</td>\n",
       "      <td>118314</td>\n",
       "      <td>1.644379e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nottheonion</td>\n",
       "      <td>Meta's threat to close down Facebook and Insta...</td>\n",
       "      <td>119017</td>\n",
       "      <td>1.644370e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>memes</td>\n",
       "      <td>Oh no anyway</td>\n",
       "      <td>94319</td>\n",
       "      <td>1.644398e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nextfuckinglevel</td>\n",
       "      <td>When everything goes right</td>\n",
       "      <td>88234</td>\n",
       "      <td>1.644372e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BlackPeopleTwitter</td>\n",
       "      <td>Granny adding insult to injury</td>\n",
       "      <td>74881</td>\n",
       "      <td>1.644357e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>interestingasfuck</td>\n",
       "      <td>The world's biggest floating crane \"Hyundai 10...</td>\n",
       "      <td>74081</td>\n",
       "      <td>1.644370e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Unexpected</td>\n",
       "      <td>Meanwhile in Japan....</td>\n",
       "      <td>69963</td>\n",
       "      <td>1.644371e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nextfuckinglevel</td>\n",
       "      <td>Steal my wave, i steal your board</td>\n",
       "      <td>71621</td>\n",
       "      <td>1.644393e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>antiwork</td>\n",
       "      <td>As it should be. 😈</td>\n",
       "      <td>65993</td>\n",
       "      <td>1.644357e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>mildlyinfuriating</td>\n",
       "      <td>Our High school covers the expiration date wit...</td>\n",
       "      <td>65632</td>\n",
       "      <td>1.644356e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nextfuckinglevel</td>\n",
       "      <td>Forklift certified</td>\n",
       "      <td>65730</td>\n",
       "      <td>1.644363e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>interestingasfuck</td>\n",
       "      <td>This cool image shows the coldest known spot i...</td>\n",
       "      <td>65383</td>\n",
       "      <td>1.644357e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>antiwork</td>\n",
       "      <td>Fucking hypocrites</td>\n",
       "      <td>65064</td>\n",
       "      <td>1.644376e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>meirl</td>\n",
       "      <td>Meirl</td>\n",
       "      <td>64547</td>\n",
       "      <td>1.644356e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>funny</td>\n",
       "      <td>Dakota has been on stall rest for months due t...</td>\n",
       "      <td>65431</td>\n",
       "      <td>1.644381e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>nextfuckinglevel</td>\n",
       "      <td>This is the Real Fair Play</td>\n",
       "      <td>67680</td>\n",
       "      <td>1.644408e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MurderedByWords</td>\n",
       "      <td>VaCcInEs CaUsE aUtIsM</td>\n",
       "      <td>64106</td>\n",
       "      <td>1.644391e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WhitePeopleTwitter</td>\n",
       "      <td>Give that man a raise now!</td>\n",
       "      <td>58573</td>\n",
       "      <td>1.644354e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>aww</td>\n",
       "      <td>her cheek squished against his shoulder is so ...</td>\n",
       "      <td>57568</td>\n",
       "      <td>1.644355e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>WhatsWrongWithYourDog</td>\n",
       "      <td>Update for those who asked. BAT HAS BEEN ADOPT...</td>\n",
       "      <td>56769</td>\n",
       "      <td>1.644360e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>antiwork</td>\n",
       "      <td>• ₊°✧︡ ˗ˏ ˋ ♡ ˎˊ ˗NO</td>\n",
       "      <td>62483</td>\n",
       "      <td>1.644416e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>FunnyAnimals</td>\n",
       "      <td>found this somewhere</td>\n",
       "      <td>54688</td>\n",
       "      <td>1.644382e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>AnimalsBeingDerps</td>\n",
       "      <td>Kitty enjoying the snow</td>\n",
       "      <td>51021</td>\n",
       "      <td>1.644357e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>iamatotalpieceofshit</td>\n",
       "      <td>Getting away with cheating on the olympic games</td>\n",
       "      <td>50999</td>\n",
       "      <td>1.644367e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>oddlysatisfying</td>\n",
       "      <td>Removing algae from water lily to help it bloom</td>\n",
       "      <td>51367</td>\n",
       "      <td>1.644403e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                subreddit                                              title  \\\n",
       "0             MadeMeSmile  Make sure the human is following our group, we...   \n",
       "1             nottheonion  Meta's threat to close down Facebook and Insta...   \n",
       "2                   memes                                       Oh no anyway   \n",
       "3        nextfuckinglevel                         When everything goes right   \n",
       "4      BlackPeopleTwitter                     Granny adding insult to injury   \n",
       "5       interestingasfuck  The world's biggest floating crane \"Hyundai 10...   \n",
       "6              Unexpected                             Meanwhile in Japan....   \n",
       "7        nextfuckinglevel                  Steal my wave, i steal your board   \n",
       "8                antiwork                                 As it should be. 😈   \n",
       "9       mildlyinfuriating  Our High school covers the expiration date wit...   \n",
       "10       nextfuckinglevel                                 Forklift certified   \n",
       "11      interestingasfuck  This cool image shows the coldest known spot i...   \n",
       "12               antiwork                                 Fucking hypocrites   \n",
       "13                  meirl                                              Meirl   \n",
       "14                  funny  Dakota has been on stall rest for months due t...   \n",
       "15       nextfuckinglevel                         This is the Real Fair Play   \n",
       "16        MurderedByWords                              VaCcInEs CaUsE aUtIsM   \n",
       "17     WhitePeopleTwitter                         Give that man a raise now!   \n",
       "18                    aww  her cheek squished against his shoulder is so ...   \n",
       "19  WhatsWrongWithYourDog  Update for those who asked. BAT HAS BEEN ADOPT...   \n",
       "20               antiwork                               • ₊°✧︡ ˗ˏ ˋ ♡ ˎˊ ˗NO   \n",
       "21           FunnyAnimals                               found this somewhere   \n",
       "22      AnimalsBeingDerps                            Kitty enjoying the snow   \n",
       "23   iamatotalpieceofshit    Getting away with cheating on the olympic games   \n",
       "24        oddlysatisfying    Removing algae from water lily to help it bloom   \n",
       "\n",
       "       ups   created_utc  \n",
       "0   118314  1.644379e+09  \n",
       "1   119017  1.644370e+09  \n",
       "2    94319  1.644398e+09  \n",
       "3    88234  1.644372e+09  \n",
       "4    74881  1.644357e+09  \n",
       "5    74081  1.644370e+09  \n",
       "6    69963  1.644371e+09  \n",
       "7    71621  1.644393e+09  \n",
       "8    65993  1.644357e+09  \n",
       "9    65632  1.644356e+09  \n",
       "10   65730  1.644363e+09  \n",
       "11   65383  1.644357e+09  \n",
       "12   65064  1.644376e+09  \n",
       "13   64547  1.644356e+09  \n",
       "14   65431  1.644381e+09  \n",
       "15   67680  1.644408e+09  \n",
       "16   64106  1.644391e+09  \n",
       "17   58573  1.644354e+09  \n",
       "18   57568  1.644355e+09  \n",
       "19   56769  1.644360e+09  \n",
       "20   62483  1.644416e+09  \n",
       "21   54688  1.644382e+09  \n",
       "22   51021  1.644357e+09  \n",
       "23   50999  1.644367e+09  \n",
       "24   51367  1.644403e+09  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the resultant dataframe:\n",
    "reddit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "The NBA has saved data on all 30 teams' shooting statistics for the 2014-2015 season here: https://stats.nba.com/js/data/sportvu/2015/shootingTeamData.json. Take a moment and look at this JSON file in your web browser. The structure of this particular JSON is complicated, but see if you can find the team-by-team data. In this problem our goal is to use `pd.json_normalize()` to get the data into a dataframe. The following questions will guide you towards this goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "Download the raw text of the NBA JSON file and register it as JSON formatted data in Python's memory. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 1181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check status of the request object:\n",
    "url3 = 'https://stats.nba.com/js/data/sportvu/2015/shootingTeamData.json'\n",
    "req3 = requests.get(url3, headers = {'User-agent': 'Dima'})\n",
    "req3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load text\n",
    "nba_json = json.loads(req3.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "Describe, in words, the path that leads to the team-by-team data. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 4.b:\n",
    "We need to look at the key `resultSets` and the first element in the corresponding list that will have `headers` with column names and `rowSet` with actual results in a team-by-team fashion. The code below returns results for the first team, Golden State Warriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1610612744',\n",
       " 'Golden State',\n",
       " 'Warriors',\n",
       " 'GSW',\n",
       " '',\n",
       " 82,\n",
       " 48.7,\n",
       " 114.9,\n",
       " 14.9,\n",
       " 0.498,\n",
       " 16.7,\n",
       " 0.645,\n",
       " 33.7,\n",
       " 0.428,\n",
       " 21.5,\n",
       " 0.418,\n",
       " 11.0,\n",
       " 11.1,\n",
       " 28.3,\n",
       " 21.5,\n",
       " 0.563,\n",
       " 21.4,\n",
       " 44.8,\n",
       " 0.478,\n",
       " 21.2,\n",
       " 42.5,\n",
       " 0.497,\n",
       " 2.3,\n",
       " 6.3,\n",
       " 0.363,\n",
       " 10.8,\n",
       " 25.3,\n",
       " 0.429]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to get to the first team\n",
    "nba_json['resultSets'][0]['rowSet'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "Use the `pd.json_normalize()` function to pull the team-by-team data into a dataframe. This is going to be tricky. You will need to use indexing on the JSON data as well as the `record_path` parameter. \n",
    "\n",
    "If you are successful, you will have a dataframe with 30 rows and 33 columns. The first row will refer to the Golden State Warriors, the second row will refer to the San Antonio Spurs, and the third row will refer to the Cleveland Cavaliers. The columns will only be named 0, 1, 2, ... at this point. (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30 entries, 0 to 29\n",
      "Data columns (total 33 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       30 non-null     object \n",
      " 1   1       30 non-null     object \n",
      " 2   2       30 non-null     object \n",
      " 3   3       30 non-null     object \n",
      " 4   4       30 non-null     object \n",
      " 5   5       30 non-null     int64  \n",
      " 6   6       30 non-null     float64\n",
      " 7   7       30 non-null     float64\n",
      " 8   8       30 non-null     float64\n",
      " 9   9       30 non-null     float64\n",
      " 10  10      30 non-null     float64\n",
      " 11  11      30 non-null     float64\n",
      " 12  12      30 non-null     float64\n",
      " 13  13      30 non-null     float64\n",
      " 14  14      30 non-null     float64\n",
      " 15  15      30 non-null     float64\n",
      " 16  16      30 non-null     float64\n",
      " 17  17      30 non-null     float64\n",
      " 18  18      30 non-null     float64\n",
      " 19  19      30 non-null     float64\n",
      " 20  20      30 non-null     float64\n",
      " 21  21      30 non-null     float64\n",
      " 22  22      30 non-null     float64\n",
      " 23  23      30 non-null     float64\n",
      " 24  24      30 non-null     float64\n",
      " 25  25      30 non-null     float64\n",
      " 26  26      30 non-null     float64\n",
      " 27  27      30 non-null     float64\n",
      " 28  28      30 non-null     float64\n",
      " 29  29      30 non-null     float64\n",
      " 30  30      30 non-null     float64\n",
      " 31  31      30 non-null     float64\n",
      " 32  32      30 non-null     float64\n",
      "dtypes: float64(27), int64(1), object(5)\n",
      "memory usage: 7.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Build the dataframe normailzing with respect o `resultSets` and `rowSet`\n",
    "nba_df = pd.json_normalize(nba_json, record_path=['resultSets', 'rowSet'])\n",
    "\n",
    "# Check the resultant dataframe:\n",
    "nba_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d\n",
    "Find the path that leads to the headers (the column names), and extract these names as a list. Then set the `.columns` attribute of the dataframe you created in part c equal to this list. The result should be that the dataframe now has the correct column names. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEAM_ID</th>\n",
       "      <th>TEAM_CITY</th>\n",
       "      <th>TEAM_NAME</th>\n",
       "      <th>TEAM_ABBREVIATION</th>\n",
       "      <th>TEAM_CODE</th>\n",
       "      <th>GP</th>\n",
       "      <th>MIN</th>\n",
       "      <th>PTS</th>\n",
       "      <th>PTS_DRIVE</th>\n",
       "      <th>FGP_DRIVE</th>\n",
       "      <th>...</th>\n",
       "      <th>CFGP</th>\n",
       "      <th>UFGM</th>\n",
       "      <th>UFGA</th>\n",
       "      <th>UFGP</th>\n",
       "      <th>CFG3M</th>\n",
       "      <th>CFG3A</th>\n",
       "      <th>CFG3P</th>\n",
       "      <th>UFG3M</th>\n",
       "      <th>UFG3A</th>\n",
       "      <th>UFG3P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1610612744</td>\n",
       "      <td>Golden State</td>\n",
       "      <td>Warriors</td>\n",
       "      <td>GSW</td>\n",
       "      <td></td>\n",
       "      <td>82</td>\n",
       "      <td>48.7</td>\n",
       "      <td>114.9</td>\n",
       "      <td>14.9</td>\n",
       "      <td>0.498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478</td>\n",
       "      <td>21.2</td>\n",
       "      <td>42.5</td>\n",
       "      <td>0.497</td>\n",
       "      <td>2.3</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.363</td>\n",
       "      <td>10.8</td>\n",
       "      <td>25.3</td>\n",
       "      <td>0.429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1610612759</td>\n",
       "      <td>San Antonio</td>\n",
       "      <td>Spurs</td>\n",
       "      <td>SAS</td>\n",
       "      <td></td>\n",
       "      <td>82</td>\n",
       "      <td>48.3</td>\n",
       "      <td>103.5</td>\n",
       "      <td>14.8</td>\n",
       "      <td>0.481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.506</td>\n",
       "      <td>18.3</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.341</td>\n",
       "      <td>6.1</td>\n",
       "      <td>15.9</td>\n",
       "      <td>0.381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1610612739</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>Cavaliers</td>\n",
       "      <td>CLE</td>\n",
       "      <td></td>\n",
       "      <td>82</td>\n",
       "      <td>48.7</td>\n",
       "      <td>104.3</td>\n",
       "      <td>16.9</td>\n",
       "      <td>0.481</td>\n",
       "      <td>...</td>\n",
       "      <td>0.473</td>\n",
       "      <td>18.2</td>\n",
       "      <td>40.7</td>\n",
       "      <td>0.447</td>\n",
       "      <td>1.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>0.299</td>\n",
       "      <td>9.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>0.378</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      TEAM_ID     TEAM_CITY  TEAM_NAME TEAM_ABBREVIATION TEAM_CODE  GP   MIN  \\\n",
       "0  1610612744  Golden State   Warriors               GSW            82  48.7   \n",
       "1  1610612759   San Antonio      Spurs               SAS            82  48.3   \n",
       "2  1610612739     Cleveland  Cavaliers               CLE            82  48.7   \n",
       "\n",
       "     PTS  PTS_DRIVE  FGP_DRIVE  ...   CFGP  UFGM  UFGA   UFGP  CFG3M  CFG3A  \\\n",
       "0  114.9       14.9      0.498  ...  0.478  21.2  42.5  0.497    2.3    6.3   \n",
       "1  103.5       14.8      0.481  ...  0.506  18.3  39.8  0.460    0.9    2.6   \n",
       "2  104.3       16.9      0.481  ...  0.473  18.2  40.7  0.447    1.7    5.7   \n",
       "\n",
       "   CFG3P  UFG3M  UFG3A  UFG3P  \n",
       "0  0.363   10.8   25.3  0.429  \n",
       "1  0.341    6.1   15.9  0.381  \n",
       "2  0.299    9.0   23.9  0.378  \n",
       "\n",
       "[3 rows x 33 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Grab column names directly from the original JSON using `.values.tolist()`\n",
    "names_list = pd.json_normalize(nba_json, record_path=['resultSets',['headers']]).values.tolist()\n",
    "\n",
    "# Flatten the list of lists using `sublist`\n",
    "flat_list = [item for sublist in names_list for item in sublist]\n",
    "nba_df.columns = flat_list\n",
    "\n",
    "# Check the resultant dataframe:\n",
    "nba_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "Save the NBA dataframe you extracted in problem 4 as a JSON-formatted text file on your local machine. Format the JSON so that it is organized as dictionary with three lists: `columns` lists the column names, `index` lists the row names, and `data` is a list-of-lists of data points, one list for each row. (Hint: this is possible with one line of code) (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store required lists in a disctionary\n",
    "nba_dict={}\n",
    "nba_dict['columns'] = nba_df.columns.to_list()\n",
    "nba_dict['index'] = nba_df.index.tolist()\n",
    "nba_dict['data'] = nba_df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store JSON object to a local text file:\n",
    "with open('data.json', 'w') as fp:\n",
    "    json.dump(nba_dict, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
