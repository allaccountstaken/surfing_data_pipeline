{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment 3: How to Load, Convert, and Write JSON Files in Python\n",
    "## DS 6001: Practice and Application of Data Science\n",
    "\n",
    "### Instructions\n",
    "Please answer the following questions as completely as possible using text, code, and the results of code as needed. Format your answers in a Jupyter notebook. To receive full credit, make sure you address every part of the problem, and make sure your document is formatted in a clean and professional way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 0\n",
    "Import the following libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import sys\n",
    "sys.tracebacklimit = 0 # turn off the error tracebacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 \n",
    "JSON and CSV are both text-based formats for the storage of data. It's possible to open either one in a plain text editor. Given this similarity, why does a CSV file usually take less memory than a JSON formatted file for the same data? Under what conditions could a JSON file be smaller in memory than a CSV file for the same data? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1: JSON tends to be bigger because all the keys are repeated in the nested structure. One likely condition when JSON may be smaller than CSV is when there are lot of missing values in the data. JSON will simply omit keys when corresponding values are missing, but CSV will still store NA or even a simple extra comma to indicate that the value is missing. When the size of the dataset is large, this will lead to noticeably larger CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "NASA has a dataset of all meteorites that have fallen to Earth between the years A.D. 860 and 2013. The data contain the name of each meteorite, along with the coordinates of the place where the meteorite hit, the mass of the meteorite, and the date of the collison. The data is stored as a JSON here: https://data.nasa.gov/resource/y77d-th95.json\n",
    "\n",
    "Look at the data in your web-browser and explain which strategy for loading the JSON into Python makes the most sense and why. \n",
    "\n",
    "Then write and run the code that will work for loading the data into Python. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2:\n",
    "Strategy:\n",
    "If the JSON has no nesting - that is, if every key is associated with a single datapoint, and no key is associated with a dictionary that contains additional features - and also does not include metadata, then use the following steps:\n",
    "\n",
    "Use requests.get() to download the raw JSON data (unless you have another way of acquiring the raw data)\n",
    "\n",
    "Use pd.read_json() on the .text attribute of the output of requests.get()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://data.nasa.gov/resource/y77d-th95.json\"\n",
    "#url1 = \"https://www.thecocktaildb.com/api/json/v1/1/filter.php?c=Cocktail\"\n",
    "req = requests.get(url, headers = {'User-agent':'Dima'})\n",
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>nametype</th>\n",
       "      <th>recclass</th>\n",
       "      <th>mass</th>\n",
       "      <th>fall</th>\n",
       "      <th>year</th>\n",
       "      <th>reclat</th>\n",
       "      <th>reclong</th>\n",
       "      <th>geolocation</th>\n",
       "      <th>:@computed_region_cbhk_fwbd</th>\n",
       "      <th>:@computed_region_nnqa_25f4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aachen</td>\n",
       "      <td>1</td>\n",
       "      <td>Valid</td>\n",
       "      <td>L5</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1880-01-01T00:00:00.000</td>\n",
       "      <td>50.77500</td>\n",
       "      <td>6.08333</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [6.08333, 50....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aarhus</td>\n",
       "      <td>2</td>\n",
       "      <td>Valid</td>\n",
       "      <td>H6</td>\n",
       "      <td>720.0</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1951-01-01T00:00:00.000</td>\n",
       "      <td>56.18333</td>\n",
       "      <td>10.23333</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [10.23333, 56...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Abee</td>\n",
       "      <td>6</td>\n",
       "      <td>Valid</td>\n",
       "      <td>EH4</td>\n",
       "      <td>107000.0</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1952-01-01T00:00:00.000</td>\n",
       "      <td>54.21667</td>\n",
       "      <td>-113.00000</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-113, 54.216...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Acapulco</td>\n",
       "      <td>10</td>\n",
       "      <td>Valid</td>\n",
       "      <td>Acapulcoite</td>\n",
       "      <td>1914.0</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1976-01-01T00:00:00.000</td>\n",
       "      <td>16.88333</td>\n",
       "      <td>-99.90000</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-99.9, 16.88...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Achiras</td>\n",
       "      <td>370</td>\n",
       "      <td>Valid</td>\n",
       "      <td>L6</td>\n",
       "      <td>780.0</td>\n",
       "      <td>Fell</td>\n",
       "      <td>1902-01-01T00:00:00.000</td>\n",
       "      <td>-33.16667</td>\n",
       "      <td>-64.95000</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-64.95, -33....</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       name   id nametype     recclass      mass  fall  \\\n",
       "0    Aachen    1    Valid           L5      21.0  Fell   \n",
       "1    Aarhus    2    Valid           H6     720.0  Fell   \n",
       "2      Abee    6    Valid          EH4  107000.0  Fell   \n",
       "3  Acapulco   10    Valid  Acapulcoite    1914.0  Fell   \n",
       "4   Achiras  370    Valid           L6     780.0  Fell   \n",
       "\n",
       "                      year    reclat    reclong  \\\n",
       "0  1880-01-01T00:00:00.000  50.77500    6.08333   \n",
       "1  1951-01-01T00:00:00.000  56.18333   10.23333   \n",
       "2  1952-01-01T00:00:00.000  54.21667 -113.00000   \n",
       "3  1976-01-01T00:00:00.000  16.88333  -99.90000   \n",
       "4  1902-01-01T00:00:00.000 -33.16667  -64.95000   \n",
       "\n",
       "                                         geolocation  \\\n",
       "0  {'type': 'Point', 'coordinates': [6.08333, 50....   \n",
       "1  {'type': 'Point', 'coordinates': [10.23333, 56...   \n",
       "2  {'type': 'Point', 'coordinates': [-113, 54.216...   \n",
       "3  {'type': 'Point', 'coordinates': [-99.9, 16.88...   \n",
       "4  {'type': 'Point', 'coordinates': [-64.95, -33....   \n",
       "\n",
       "   :@computed_region_cbhk_fwbd  :@computed_region_nnqa_25f4  \n",
       "0                          NaN                          NaN  \n",
       "1                          NaN                          NaN  \n",
       "2                          NaN                          NaN  \n",
       "3                          NaN                          NaN  \n",
       "4                          NaN                          NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nasa_json = pd.read_json(req.text)\n",
    "nasa_json.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "The textbook chapter for this module shows, as an example, how to pull data in JSON format from Reddit's top 25 posts on [/r/popular](https://www.reddit.com/r/popular/top/). The steps outlined there pull all of the features in the data into the dataframe, resulting in a dataframe with 172 columns. \n",
    "\n",
    "If we only wanted a few features, then looping across elements of the JSON list itself and extracting only the data we want may be a more efficient approach.\n",
    "\n",
    "Use looping - and not `pd.read_json()` or `pd.json_normalize()` - to create a dataframe with 25 rows (one for each of the top 25 posts), and only columns for `subreddit`, `title`, `ups`, and `created_utc`. The JSON file exists at http://www.reddit.com/r/popular/top.json, and don't forget to specify `headers = {'User-agent': 'DS6001'}` within `requests.get()`. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "The NBA has saved data on all 30 teams' shooting statistics for the 2014-2015 season here: https://stats.nba.com/js/data/sportvu/2015/shootingTeamData.json. Take a moment and look at this JSON file in your web browser. The structure of this particular JSON is complicated, but see if you can find the team-by-team data. In this problem our goal is to use `pd.json_normalize()` to get the data into a dataframe. The following questions will guide you towards this goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a\n",
    "Download the raw text of the NBA JSON file and register it as JSON formatted data in Python's memory. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b\n",
    "Describe, in words, the path that leads to the team-by-team data. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c\n",
    "Use the `pd.json_normalize()` function to pull the team-by-team data into a dataframe. This is going to be tricky. You will need to use indexing on the JSON data as well as the `record_path` parameter. \n",
    "\n",
    "If you are successful, you will have a dataframe with 30 rows and 33 columns. The first row will refer to the Golden State Warriors, the second row will refer to the San Antonio Spurs, and the third row will refer to the Cleveland Cavaliers. The columns will only be named 0, 1, 2, ... at this point. (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d\n",
    "Find the path that leads to the headers (the column names), and extract these names as a list. Then set the `.columns` attribute of the dataframe you created in part c equal to this list. The result should be that the dataframe now has the correct column names. (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5\n",
    "Save the NBA dataframe you extracted in problem 4 as a JSON-formatted text file on your local machine. Format the JSON so that it is organized as dictionary with three lists: `columns` lists the column names, `index` lists the row names, and `data` is a list-of-lists of data points, one list for each row. (Hint: this is possible with one line of code) (2 points)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
